# mini-vLLM

[![Python 3.1+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-yellow.svg)](https://opensource.org/licenses/Apache-2.0)

ğŸš€ **è½»é‡çº§é«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“**

mini-vLLM æ˜¯ä¸€ä¸ªä»é›¶å¼€å§‹æ„å»ºçš„è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“ï¼Œç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªç®€å•ã€é«˜æ•ˆã€å¯æ‰©å±•çš„æ¨ç†è§£å†³æ–¹æ¡ˆï¼Œ å¸®åŠ©å¼€å‘è€…å¿«é€Ÿç†è§£å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†åŸç†ã€‚mini-vLLM ä¸æ˜¯ vLLM çš„ç›´æ¥æ›¿ä»£å“ï¼Œè€Œæ˜¯ä¸€ä¸ªä¸“æ³¨äºç†è§£å’Œå­¦ä¹ çš„é¡¹ç›®ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ”¥ é«˜æ€§èƒ½æ¨ç†

- **CUDA Graph ä¼˜åŒ–**: å‡å°‘ decode é˜¶æ®µçš„è°ƒåº¦å¼€é”€ï¼Œæå‡æ¨ç†é€Ÿåº¦
- **æ™ºèƒ½æ‰¹å¤„ç†**: æ”¯æŒåŠ¨æ€æ‰¹å¤„ç†å’Œåºåˆ—é•¿åº¦æ„ŸçŸ¥è°ƒåº¦
- **KV ç¼“å­˜ä¼˜åŒ–**: å—å¼ KV ç¼“å­˜ç®¡ç†å’Œå‰ç¼€ç¼“å­˜ï¼Œå‡å°‘å†…å­˜å ç”¨

### ğŸ§  å…ˆè¿›æ¶æ„

- **å¼ é‡å¹¶è¡Œ**: æ”¯æŒå¤š GPU åˆ†å¸ƒå¼æ¨ç†ï¼ˆ1-8 å¼ é‡å¹¶è¡Œåº¦ï¼‰
- **ä¸¤é˜¶æ®µè°ƒåº¦**: é¢„å¡«å……ï¼ˆPrefillï¼‰å’Œè§£ç ï¼ˆDecodeï¼‰é˜¶æ®µåˆ†ç¦»è°ƒåº¦
- **FlashAttention**: é›†æˆé«˜æ€§èƒ½æ³¨æ„åŠ›è®¡ç®—åº“

### ğŸ› ï¸ çµæ´»é…ç½®
- **å†…å­˜ç®¡ç†**: å¯é…ç½® GPU å†…å­˜åˆ©ç”¨ç‡ï¼ˆ10%-100%ï¼‰
- **ç¼“å­˜é…ç½®**: å¯è°ƒèŠ‚ KV ç¼“å­˜å—å¤§å°å’Œæ•°é‡
- **é‡‡æ ·æ§åˆ¶**: æ”¯æŒæ¸©åº¦è°ƒèŠ‚å’Œè‡ªå®šä¹‰é‡‡æ ·å‚æ•°


## ğŸ“¦ å®‰è£…

### ç³»ç»Ÿè¦æ±‚

- Python 3.10+
- CUDA 11.8+ (æ¨è)
- PyTorch 2.0+
- è‡³å°‘ 8GB GPU å†…å­˜

### å®‰è£…ä¾èµ–

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/jianzhnie/mini-vllm.git
cd mini-vllm

# å®‰è£…ä¾èµ–
pip install -e .

# å¯é€‰ï¼šå®‰è£… FlashAttention (æ˜¾è‘—æå‡æ€§èƒ½)
pip install flash-attn --no-build-isolation

# å¯é€‰ï¼šå®‰è£… Triton (å¯ç”¨æ›´å¤šä¼˜åŒ–)
pip install triton
```

### éªŒè¯å®‰è£…

```bash
python -c "import minivllm; print('Installation successful!')"
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from minivllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    max_num_seqs=256,
    max_num_batched_tokens=8192,
    gpu_memory_utilization=0.9
)

# å‡†å¤‡è¾“å…¥
prompts = [
    "Once upon a time",
    "The future of AI is",
    "In the deep learning era"
]

# é…ç½®é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    max_tokens=128,
    ignore_eos=False
)

# ç”Ÿæˆæ–‡æœ¬
outputs = llm.generate(prompts, sampling_params)

# æ‰“å°ç»“æœ
for output in outputs:
    print(f"Prompt: {output['prompt']}")
    print(f"Generated: {output['text']}")
    print("-" * 50)
```

### é«˜çº§é…ç½®

```python
# å¤š GPU é…ç½®
llm = LLM(
    model="meta-llama/Llama-2-70b-chat-hf",
    tensor_parallel_size=4,  # ä½¿ç”¨ 4 å¼  GPU
    max_num_seqs=512,
    max_num_batched_tokens=16384,
    gpu_memory_utilization=0.85,
    enforce_eager=False,  # ä½¿ç”¨ CUDA Graph
    max_model_len=4096
)

# è‡ªå®šä¹‰é‡‡æ ·å‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,  # æ›´é«˜æ¸©åº¦ = æ›´å¤šåˆ›æ„
    max_tokens=256,
    ignore_eos=True  # å¿½ç•¥ç»“æŸç¬¦ï¼Œç»§ç»­ç”Ÿæˆ
)

# æµå¼ç”Ÿæˆï¼ˆå¦‚æœæ”¯æŒï¼‰
for output in llm.generate(prompts, sampling_params):
    print(output['text'], end="", flush=True)
```

## âš™ï¸ é…ç½®å‚æ•°

### LLM é…ç½®é€‰é¡¹

| å‚æ•°                     | ç±»å‹  | é»˜è®¤å€¼ | è¯´æ˜                               |
| ------------------------ | ----- | ------ | ---------------------------------- |
| `model`                  | str   | å¿…éœ€   | æ¨¡å‹è·¯å¾„ï¼ˆHuggingFace æ ¼å¼ï¼‰       |
| `max_num_seqs`           | int   | 512    | æ¯æ‰¹æ¬¡æœ€å¤§åºåˆ—æ•°                   |
| `max_num_batched_tokens` | int   | 16384  | æ¯æ‰¹æ¬¡æœ€å¤§ token æ•°                |
| `max_model_len`          | int   | 4096   | æ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦                 |
| `gpu_memory_utilization` | float | 0.9    | GPU å†…å­˜åˆ©ç”¨ç‡ (0.1-1.0)           |
| `tensor_parallel_size`   | int   | 1      | å¼ é‡å¹¶è¡Œåº¦ (1-8)                   |
| `enforce_eager`          | bool  | False  | å¼ºåˆ¶ä½¿ç”¨ eager æ¨¡å¼                |
| `kvcache_block_size`     | int   | 256    | KV ç¼“å­˜å—å¤§å°ï¼ˆå¿…é¡»èƒ½è¢« 256 æ•´é™¤ï¼‰ |
| `num_kvcache_blocks`     | int   | -1     | KV ç¼“å­˜å—æ•°é‡ï¼ˆ-1 è¡¨ç¤ºè‡ªåŠ¨ï¼‰       |

### SamplingParams é…ç½®

| å‚æ•°          | ç±»å‹  | é»˜è®¤å€¼ | è¯´æ˜                 |
| ------------- | ----- | ------ | -------------------- |
| `temperature` | float | 1.0    | é‡‡æ ·æ¸©åº¦ (1e-10-2.0) |
| `max_tokens`  | int   | 64     | ç”Ÿæˆçš„æœ€å¤§ token æ•°  |
| `ignore_eos`  | bool  | False  | æ˜¯å¦å¿½ç•¥ç»“æŸç¬¦       |


## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº [Apache 2.0 è®¸å¯è¯](LICENSE) å¼€æºã€‚

## ğŸ¤ è‡´è°¢

éå¸¸æ„Ÿè°¢ [nanovllm](https://github.com/GeeeekExplorer/nano-vllm)ï¼Œ å®ƒä¸º mini-vLLM æä¾›äº†å®è´µçš„è®¾è®¡æ€è·¯å’Œå®ç°å‚è€ƒã€‚ä»ä¸­å€Ÿé‰´äº†è®¸å¤šå†…å­˜ç®¡ç†å’Œè°ƒåº¦æœºåˆ¶ç›¸å…³ä»£ç ï¼Œå¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“çš„å®ç°ç»†èŠ‚ã€‚ åŒæ—¶ï¼Œæ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®å’Œåº“ï¼Œå®ƒä»¬ä¸º mini-vLLM çš„å¼€å‘æä¾›äº†é‡è¦æ”¯æŒï¼š

- [vLLM](https://github.com/vllm-project/vllm) - çµæ„Ÿæ¥æº
- [Transformers](https://github.com/huggingface/transformers) - æ¨¡å‹å®šä¹‰å’ŒåŠ è½½
- [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æ¶
- [FlashAttention](https://github.com/HazyResearch/flash-attention) - é«˜æ€§èƒ½æ³¨æ„åŠ›
- [Triton](https://github.com/openai/triton) - GPU å†…æ ¸ä¼˜åŒ–


<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼ â­**

</div>
