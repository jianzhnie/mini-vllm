# NPU Flash Attention ä½¿ç”¨æŒ‡å—

## 1. å¿«é€Ÿå…¥é—¨

### 1.1 ä»€ä¹ˆæ˜¯ NPU Flash Attention

NPU Flash Attention æ˜¯åä¸ºæ˜‡è…¾ (Ascend) NPU æä¾›çš„é«˜æ€§èƒ½æ³¨æ„åŠ›è®¡ç®—ç®—å­åº“ï¼Œä¸“ä¸º Transformer æ¨¡å‹çš„è‡ªæ³¨æ„åŠ›è®¡ç®—è€Œè®¾è®¡ã€‚åŸºäº FlashAttention ç®—æ³•æ€æƒ³ï¼Œé€šè¿‡ç¡¬ä»¶çº§ä¼˜åŒ–å®ç°æ˜¾è‘—æ€§èƒ½æå‡ï¼š

- ğŸš€ **æ€§èƒ½æå‡**: ç›¸æ¯”æ ‡å‡†å®ç°æå‡ 2-4 å€
- ğŸ’¾ **å†…å­˜ä¼˜åŒ–**: FlashAttention ç®—æ³•é™ä½å†…å­˜å ç”¨è‡³ O(N)
- ğŸ”§ **å¤šåœºæ™¯æ”¯æŒ**: è®­ç»ƒã€æ¨ç†ã€é‡åŒ–å…¨åœºæ™¯è¦†ç›–
- âš¡ **ç¡¬ä»¶åŠ é€Ÿ**: å……åˆ†åˆ©ç”¨æ˜‡è…¾ NPU ç®—åŠ›å’Œå­˜å‚¨å±‚æ¬¡

### 1.2 ç¯å¢ƒè¦æ±‚

| ç»„ä»¶ | æœ€ä½ç‰ˆæœ¬ | æ¨èç‰ˆæœ¬ | è¯´æ˜ |
|------|----------|----------|------|
| **ç¡¬ä»¶å¹³å°** | Ascend 910B | Ascend 910B/A2/A3 | æ¨èä½¿ç”¨è®­ç»ƒç³»åˆ— |
| **æ“ä½œç³»ç»Ÿ** | Linux 3.10+ | Ubuntu 20.04/22.04 | æ”¯æŒä¸»æµ Linux å‘è¡Œç‰ˆ |
| **CANN** | 7.0 | 8.0.RC1+ | æ˜‡è…¾è®¡ç®—æ¶æ„ |
| **PyTorch** | 2.1.0 | 2.3.0+ | åŒ…å« torch_npu æ‰©å±• |
| **Python** | 3.8 | 3.9/3.10 | å…¼å®¹æ€§æ›´å¥½ |

> ğŸ’¡ **å¿«é€ŸéªŒè¯ç¯å¢ƒ**:
> ```python
> import torch
> import torch_npu
> print(f"PyTorch: {torch.__version__}")
> print(f"NPUå¯ç”¨: {torch.npu.is_available()}")
> ```

### 1.3 API ä¸€è§ˆ

NPU Flash Attention æä¾›å®Œæ•´çš„æ³¨æ„åŠ›è®¡ç®—è§£å†³æ–¹æ¡ˆï¼ŒæŒ‰ä½¿ç”¨åœºæ™¯åˆ†ä¸ºä¸¤å¤§ç±»ï¼š

#### è®­ç»ƒåœºæ™¯
```python
torch_npu.npu_fusion_attention  # èåˆæ³¨æ„åŠ›ï¼Œæ”¯æŒå˜é•¿åºåˆ—
```

#### æ¨ç†åœºæ™¯
```python
torch_npu.npu_incre_flash_attention        # å¢é‡è§£ç  (å•token)
torch_npu.npu_prompt_flash_attention       # é¦–æ¬¡å¤„ç† (å¤štokens)
torch_npu.npu_fused_infer_attention_score  # ç»Ÿä¸€æ¨ç†æ¥å£ â­æ¨è
torch_npu.npu_advance_step_flashattn       # vLLMä¸“ç”¨æ¥å£
```

## 2. æ ¸å¿ƒAPIè¯¦è§£

### 2.1 è®­ç»ƒåœºæ™¯: `npu_fusion_attention`

é€‚ç”¨äºæ¨¡å‹è®­ç»ƒé˜¶æ®µï¼Œæ”¯æŒå®Œæ•´çš„æ³¨æ„åŠ›è®¡ç®—å’Œæ¢¯åº¦å›ä¼ ã€‚

#### å‡½æ•°ç­¾å
```python
torch_npu.npu_fusion_attention(
    query,               # [B,S,N,D] æˆ– [T,N,D]
    key,                 # åŒ query
    value,               # åŒ query
    head_num,            # æ³¨æ„åŠ›å¤´æ•°
    input_layout,        # æ•°æ®å¸ƒå±€: "BSNH"/"BNSD"/"TND"
    pse=None,            # ä½ç½®ç¼–ç åç§»
    atten_mask=None,     # æ³¨æ„åŠ›æ©ç 
    scale=1.0,           # ç¼©æ”¾å› å­ï¼Œæ¨è 1/âˆšD
    keep_prob=1.0,       # Dropout æ¦‚ç‡
    sparse_mode=0,       # ç¨€ç–æ¨¡å¼ (0-8)
    # ... å…¶ä»–é«˜çº§å‚æ•°
) â†’ (output, softmax_max, softmax_sum, ...)
```

#### æ ¸å¿ƒå‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | æ¨èå€¼ | è¯´æ˜ |
|------|------|--------|------|
| `input_layout` | str | `"BNSD"` | æ‰¹é‡Ã—å¤´æ•°Ã—åºåˆ—Ã—ç»´åº¦ï¼ŒNPUæœ€ä¼˜ |
| `scale` | float | `1.0/math.sqrt(head_dim)` | æ ‡å‡†ç¼©æ”¾å› å­ |
| `sparse_mode` | int | `3` | å³ä¸‹å› æœæ©ç ï¼Œé€‚åˆ GPT ç±»æ¨¡å‹ |
| `keep_prob` | float | `0.9` (è®­ç»ƒ) / `1.0` (æ¨ç†) | Dropout ä¿ç•™æ¦‚ç‡ |

#### ä½¿ç”¨ç¤ºä¾‹
```python
import torch
import torch_npu
import math

# åŸºç¡€è®¾ç½®
batch_size, seq_len, num_heads, head_dim = 2, 512, 8, 64
scale = 1.0 / math.sqrt(head_dim)

# å‡†å¤‡æ•°æ® (æ¨è BNSD å¸ƒå±€)
query = torch.randn(batch_size, num_heads, seq_len, head_dim, dtype=torch.float16).npu()
key = torch.randn(batch_size, num_heads, seq_len, head_dim, dtype=torch.float16).npu()
value = torch.randn(batch_size, num_heads, seq_len, head_dim, dtype=torch.float16).npu()

# è°ƒç”¨èåˆæ³¨æ„åŠ›
output, softmax_max, softmax_sum, _, _, _, _ = torch_npu.npu_fusion_attention(
    query, key, value,
    head_num=num_heads,
    input_layout="BNSD",
    scale=scale,
    keep_prob=0.9,  # è®­ç»ƒæ—¶å¯ç”¨ dropout
    sparse_mode=3   # å› æœæ©ç 
)

print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # [2, 8, 512, 64]
```

#### å˜é•¿åºåˆ—æ”¯æŒ
```python
# TND å¸ƒå±€ç”¨äºå˜é•¿åºåˆ—
total_tokens = 1000  # æ‰¹é‡ä¸­æ‰€æœ‰tokenæ€»æ•°
query = torch.randn(total_tokens, num_heads, head_dim, dtype=torch.float16).npu()
key = torch.randn(total_tokens, num_heads, head_dim, dtype=torch.float16).npu()
value = torch.randn(total_tokens, num_heads, head_dim, dtype=torch.float16).npu()

# å®é™…åºåˆ—é•¿åº¦ (éç´¯ç§¯)
actual_seq_qlen = [100, 200, 150, 550]  # 4ä¸ªåºåˆ—çš„å®é™…é•¿åº¦
actual_seq_kvlen = [100, 200, 150, 550]

output, *_ = torch_npu.npu_fusion_attention(
    query, key, value,
    head_num=num_heads,
    input_layout="TND",  # å˜é•¿åºåˆ—ä¸“ç”¨å¸ƒå±€
    scale=scale,
    actual_seq_qlen=actual_seq_qlen,
    actual_seq_kvlen=actual_seq_kvlen,
    sparse_mode=0
)
```

### 2.2 æ¨ç†åœºæ™¯: ç»Ÿä¸€æ¥å£ `npu_fused_infer_attention_score`

**æ¨èä½¿ç”¨** - è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¡ç®—åˆ†æ”¯ï¼ŒåŒæ—¶æ”¯æŒ prefill å’Œ decode é˜¶æ®µã€‚

#### è‡ªé€‚åº”é€»è¾‘
```mermaid
graph TD
    A[Queryåºåˆ—é•¿åº¦=1?] -->|æ˜¯| B[å¢é‡åˆ†æ”¯<br/>npu_incre_flash_attention]
    A -->|å¦| C[å…¨é‡åˆ†æ”¯<br/>npu_prompt_flash_attention]
    B --> D[è¾“å‡º Attention ç»“æœ]
    C --> D
```

#### å‡½æ•°ç­¾å
```python
torch_npu.npu_fused_infer_attention_score(
    query, key_cache, value_cache,
    *,
    num_heads,
    scale_value,
    input_layout="BNSD",
    actual_seq_lengths=None,      # å„åºåˆ—æœ‰æ•ˆé•¿åº¦
    actual_seq_lengths_kv=None,   # KVåºåˆ—é•¿åº¦
    sparse_mode=3,                # å› æœæ©ç 
    pre_tokens=65535,             # å‘å‰å¯è§tokenæ•°
    next_tokens=0,                # å‘åå¯è§tokenæ•°
    softmax_lse_flag=False        # æ˜¯å¦è¿”å›log-sum-exp
) â†’ (attention_output, [optional] lse)
```

#### å®Œæ•´æ¨ç†ç¤ºä¾‹
```python
import torch
import torch_npu
import math

class NPUAttentionEngine:
    """NPU Flash Attention æ¨ç†å¼•æ“"""

    def __init__(self, num_heads: int, head_dim: int):
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.scale = 1.0 / math.sqrt(head_dim)

    def unified_inference(self, query, key_cache, value_cache, seq_length):
        """ç»Ÿä¸€æ¨ç†æ¥å£ - è‡ªåŠ¨é€‰æ‹© prefill/decode"""
        return torch_npu.npu_fused_infer_attention_score(
            query, key_cache, value_cache,
            num_heads=self.num_heads,
            scale_value=self.scale,
            input_layout="BNSD",
            actual_seq_lengths=[seq_length],
            actual_seq_lengths_kv=[seq_length],
            sparse_mode=3,  # causal mask
            pre_tokens=65535,
            next_tokens=0,
            softmax_lse_flag=True  # è·å–æ•°å€¼ç¨³å®šæ€§ä¿¡æ¯
        )

# ä½¿ç”¨ç¤ºä¾‹
engine = NPUAttentionEngine(num_heads=8, head_dim=64)

# åˆå§‹åŒ– KV Cache
max_kv_len = 1024
key_cache = torch.randn(1, max_kv_len, 8, 64, dtype=torch.float16).npu()
value_cache = torch.randn(1, max_kv_len, 8, 64, dtype=torch.float16).npu()

# Prefill é˜¶æ®µ - å¤„ç†å®Œæ•´prompt
prefill_query = torch.randn(1, 64, 8, 64, dtype=torch.float16).npu()  # 64ä¸ªtoken
prefill_out, prefill_lse = engine.unified_inference(
    prefill_query, key_cache[:, :64], value_cache[:, :64], 64
)

# Decode é˜¶æ®µ - é€tokenç”Ÿæˆ
decode_query = torch.randn(1, 1, 8, 64, dtype=torch.float16).npu()  # 1ä¸ªtoken
decode_out, decode_lse = engine.unified_inference(
    decode_query, key_cache, value_cache, 65  # 64+1=65
)
```

### 2.3 ä¸“ç”¨æ¨ç†æ¥å£

#### `npu_prompt_flash_attention` - é¦–æ¬¡å¤„ç†
```python
# ä¸“é—¨ç”¨äº prefill é˜¶æ®µ
prefill_out = torch_npu.npu_prompt_flash_attention(
    query, key, value,
    num_heads=8,
    scale_value=scale,
    input_layout="BNSD",
    sparse_mode=3,  # causal mask
    pre_tokens=65535,
    next_tokens=0
)
```

#### `npu_incre_flash_attention` - å¢é‡è§£ç 
```python
# ä¸“é—¨ç”¨äº decode é˜¶æ®µ (éœ€è¦å›¾æ¨¡å¼)
decode_out = torch_npu.npu_incre_flash_attention(
    query, key_cache, value_cache,
    num_heads=8,
    scale_value=scale,
    input_layout="BNSD",
    actual_seq_lengths=[current_length]
)
```

#### `npu_advance_step_flashattn` - vLLMä¸“ç”¨
```python
# vLLM é£æ ¼çš„step attention
torch_npu.npu_advance_step_flashattn(
    input_tokens, sampled_token_ids, positions,
    seq_lengths, slot_mapping, block_tables,
    num_seqs, num_queries, block_size
)
```

## 3. ç‰ˆæœ¬å…¼å®¹æ€§ä¸æ¼”è¿›

### 3.1 API å‘å±•å†ç¨‹

| ç‰ˆæœ¬ | æ–°å¢API | é‡è¦ç‰¹æ€§ | é€‚ç”¨åœºæ™¯ |
|------|---------|----------|----------|
| **PyTorch 2.1** | `fusion_attention`, `incre_flash_attention` | åŸºç¡€è®­ç»ƒ/æ¨ç†æ”¯æŒ | ä¼ ç»Ÿåœºæ™¯ |
| **PyTorch 2.3+** | `prompt_flash_attention`, `fused_infer_attention_score` | ç»Ÿä¸€æ¨ç†æ¥å£ | æ¨èç”Ÿäº§ä½¿ç”¨ |
| **PyTorch 2.5+** | `advance_step_flashattn` | vLLMé›†æˆ, PageAttention | é«˜çº§æœåŠ¡éƒ¨ç½² |

### 3.2 ç¡¬ä»¶æ”¯æŒçŸ©é˜µ

| ç¡¬ä»¶å‹å· | è®­ç»ƒ | æ¨ç† | é‡åŒ– | PageAttention | æ¨èç”¨é€” |
|----------|------|------|------|---------------|----------|
| **Atlas 200I A2** | âŒ | âœ… | åŸºç¡€ | âŒ | è¾¹ç¼˜æ¨ç† |
| **Atlas 300I A2** | âŒ | âœ… | å®Œå–„ | âŒ | äº‘ç«¯æ¨ç† |
| **Atlas 300T A2** | âœ… | âœ… | å®Œå–„ | éƒ¨åˆ† | è®­ç»ƒæ¨ç† |
| **Atlas 800 A2** | âœ… | âœ… | å®Œå–„ | âœ… | ä¼ä¸šçº§è®­ç»ƒ |
| **Atlas 900 A3** | âœ… | âœ… | æœ€ä¼˜ | âœ… | è¶…å¤§è§„æ¨¡ |

### 3.3 å‡çº§æŒ‡å—

#### ä» 2.1 å‡çº§åˆ° 2.3+
```python
# æ—§ç‰ˆæœ¬ (2.1)
def old_inference(query, key_cache, value_cache, seq_len):
    if query.shape[1] == 1:  # å•token
        return torch_npu.npu_incre_flash_attention(query, key_cache, value_cache)
    else:
        raise NotImplementedError("Prefill not supported")

# æ–°ç‰ˆæœ¬ (2.3+) - æ¨è
def new_inference(query, key_cache, value_cache, seq_len):
    return torch_npu.npu_fused_infer_attention_score(
        query, key_cache, value_cache,
        actual_seq_lengths=[seq_len],
        sparse_mode=3  # è‡ªåŠ¨é€‰æ‹©åˆ†æ”¯
    )
```

## 4. é«˜çº§åŠŸèƒ½ä¸ä¼˜åŒ–

### 4.1 ç¨€ç–æ¨¡å¼è¯¦è§£

NPU Flash Attention æä¾› 8 ç§ç¨€ç–æ¨¡å¼ï¼Œé’ˆå¯¹ä¸åŒåœºæ™¯ä¼˜åŒ–ï¼š

| æ¨¡å¼ | åç§° | é€‚ç”¨åœºæ™¯ | è¯´æ˜ |
|------|------|----------|------|
| `0` | DefaultMask | é€šç”¨åœºæ™¯ | æ ¹æ® atten_mask åˆ¤æ–­ |
| `1` | AllMask | å®Œæ•´æ³¨æ„åŠ› | æ— æ©ç é™åˆ¶ |
| `2` | LeftUpCausal | å› æœå˜ä½“ | å·¦ä¸Šå¯¹é½çš„å› æœæ©ç  |
| `3` | RightDownCausal | **æ¨è** | æ ‡å‡†å³ä¸‹å› æœæ©ç  |
| `4` | Band | å±€éƒ¨æ³¨æ„åŠ› | å¸¦å®½æ©ç ï¼Œé€‚åˆé•¿åºåˆ— |
| `5` | Prefix | å‰ç¼€æ¨¡å¼ | éå‹ç¼©å‰ç¼€æ³¨æ„åŠ› |
| `6` | Prefix | å‰ç¼€æ¨¡å¼ | å‹ç¼©å‰ç¼€æ³¨æ„åŠ› |
| `7` | Varlen | å˜é•¿ä¼˜åŒ– | åŸºäº mode3 çš„å˜é•¿åºåˆ— |
| `8` | Varlen | å˜é•¿ä¼˜åŒ– | åŸºäº mode2 çš„å˜é•¿åºåˆ— |

```python
# æ¨èé…ç½®ç¤ºä¾‹
configs = {
    "gpt_training": {"sparse_mode": 3, "pre_tokens": 65535, "next_tokens": 0},
    "bert_training": {"sparse_mode": 0, "atten_mask": segment_mask},
    "local_attention": {"sparse_mode": 4, "pre_tokens": 128, "next_tokens": 128},
    "prefix_lm": {"sparse_mode": 5, "pre_tokens": 65535, "next_tokens": 128}
}
```

### 4.2 é‡åŒ–æ¨ç†

æ”¯æŒ FP16â†’INT8/FP8 é‡åŒ–ï¼Œæ˜¾è‘—å‡å°‘å†…å­˜å ç”¨ï¼š

```python
def quantized_inference():
    """é‡åŒ–æ¨ç†ç¤ºä¾‹"""
    batch_size, seq_len, num_heads, head_dim = 1, 1, 8, 64

    # Query ä¿æŒ FP16ï¼ŒKV ä½¿ç”¨ INT8
    query = torch.randn(batch_size, seq_len, num_heads, head_dim, dtype=torch.float16).npu()
    key_int8 = torch.randint(-128, 127, (1, 100, num_heads, head_dim), dtype=torch.int8).npu()
    value_int8 = torch.randint(-128, 127, (1, 100, num_heads, head_dim), dtype=torch.int8).npu()

    # é‡åŒ–å‚æ•°
    dequant_scale = torch.tensor(1.0/127.0, dtype=torch.float32).npu()
    quant_scale = torch.tensor(127.0, dtype=torch.float32).npu()
    quant_offset = torch.tensor(0.0, dtype=torch.float32).npu()

    # é‡åŒ–æ¨ç†
    output = torch_npu.npu_incre_flash_attention(
        query, key_int8, value_int8,
        dequant_scale1=dequant_scale,    # ç¬¬ä¸€å±‚åé‡åŒ–
        quant_scale2=quant_scale,        # ç¬¬äºŒå±‚é‡åŒ–
        quant_offset2=quant_offset,     # ç¬¬äºŒå±‚åç§»
        num_heads=num_heads,
        scale_value=1.0 / math.sqrt(head_dim)
    )

    return output  # å†…å­˜å ç”¨å‡å°‘çº¦ 50%
```

### 4.3 PageAttention ä¸ KV Cache ä¼˜åŒ–

é€‚ç”¨äºé«˜å¹¶å‘æ¨ç†åœºæ™¯ï¼Œé€šè¿‡åˆ†å—ç®¡ç†ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼š

```python
class PageAttentionManager:
    """PageAttention KV Cache ç®¡ç†å™¨"""

    def __init__(self, block_size=16):
        self.block_size = block_size

    def allocate_blocks(self, max_blocks_per_seq=64):
        """åˆ†é…å—æ˜ å°„è¡¨"""
        num_seqs = 4
        block_tables = torch.full((num_seqs, max_blocks_per_seq), -1, dtype=torch.int64).npu()

        # ä¸ºæ¯ä¸ªåºåˆ—åˆ†é…ç‰©ç†å—
        for i in range(num_seqs):
            for j in range(max_blocks_per_seq):
                block_tables[i, j] = i * max_blocks_per_seq + j

        return block_tables

    def attention_with_blocks(self, query, key_cache, value_cache,
                              seq_lengths, block_tables):
        """ä½¿ç”¨ PageAttention çš„æ³¨æ„åŠ›è®¡ç®—"""
        return torch_npu.npu_incre_flash_attention(
            query, key_cache, value_cache,
            block_table=block_tables,
            actual_seq_lengths=seq_lengths,
            block_size=self.block_size,
            num_heads=self.num_heads,
            scale_value=self.scale,
            input_layout="BNSD"
        )

# ä½¿ç”¨ç¤ºä¾‹
manager = PageAttentionManager(block_size=16)
block_tables = manager.allocate_blocks()

# æ›´é«˜æ•ˆçš„ KV Cache ç®¡ç†ï¼Œå†…å­˜åˆ©ç”¨ç‡æå‡ 60%+
output = manager.attention_with_blocks(
    query, key_cache, value_cache,
    seq_lengths=torch.tensor([64, 128, 256, 512]),
    block_tables=block_tables
)
```

### 4.4 æ€§èƒ½è°ƒä¼˜æœ€ä½³å®è·µ

#### æ•°æ®å¸ƒå±€ä¼˜åŒ–
```python
def optimize_layout(query, key, value):
    """æ•°æ®å¸ƒå±€ä¼˜åŒ–æŒ‡å—"""

    # âœ… æ¨è: BNSD å¸ƒå±€ - NPU å†…éƒ¨æœ€ä¼˜
    if query.shape[1] == query.shape[-1]:  # æ£€æŸ¥æ˜¯å¦ä¸º BNSD
        return query, key, value

    # âŒ é¿å…: BSH å¸ƒå±€ - éœ€è¦è½¬æ¢
    if query.dim() == 3 and query.shape[-1] % query.shape[1] == 0:
        num_heads = query.shape[1]
        head_dim = query.shape[-1] // num_heads
        batch_size, seq_len = query.shape[0], query.shape[-1] // (num_heads * head_dim)

        query = query.view(batch_size, num_heads, seq_len, head_dim)
        key = key.view(batch_size, num_heads, seq_len, head_dim)
        value = value.view(batch_size, num_heads, seq_len, head_dim)

    return query.contiguous(), key.contiguous(), value.contiguous()
```

#### å†…å­˜ä¼˜åŒ–ç­–ç•¥
```python
def memory_efficient_attention(query, key, value, **kwargs):
    """å†…å­˜ä¼˜åŒ–ç­–ç•¥"""

    # 1. æ¢¯åº¦æ£€æŸ¥ç‚¹ - è®­ç»ƒæ—¶å‡å°‘æ˜¾å­˜
    if kwargs.get('training', False):
        return torch.utils.checkpoint.checkpoint(
            torch_npu.npu_fusion_attention,
            query, key, value,
            use_reentrant=False,
            **kwargs
        )

    # 2. åˆ†å—å¤„ç† - é•¿åºåˆ—
    seq_len = query.shape[2] if query.dim() == 4 else query.shape[1]
    if seq_len > 4096:
        return chunked_attention(query, key, value, chunk_size=2048)

    # 3. é‡åŒ–æ¨ç† - å†…å­˜å—é™
    if kwargs.get('quantize', False):
        return quantized_inference(query, key, value)

    # æ ‡å‡†è®¡ç®—
    return torch_npu.npu_fusion_attention(query, key, value, **kwargs)
```

## 5. å®é™…åº”ç”¨åœºæ™¯

### 5.1 å¤§è¯­è¨€æ¨¡å‹æ¨ç†

#### åœºæ™¯ç‰¹ç‚¹
- æ”¯æŒ 7B-70B å‚æ•°è§„æ¨¡æ¨¡å‹
- é«˜å¹¶å‘è¯·æ±‚å¤„ç†
- ä½å»¶è¿Ÿè¦æ±‚

#### å®ç°æ–¹æ¡ˆ
```python
class LLMInferenceService:
    """å¤§è¯­è¨€æ¨¡å‹æ¨ç†æœåŠ¡"""

    def __init__(self, model_config):
        self.attention = NPUAttentionEngine(
            num_heads=model_config.num_heads,
            head_dim=model_config.head_dim
        )
        self.kv_cache = KVCacheManager(
            max_batch_size=model_config.max_batch_size,
            max_seq_len=model_config.max_seq_len,
            block_size=16
        )

    def generate_batch(self, input_ids_list, max_new_tokens=100):
        """æ‰¹é‡ç”Ÿæˆ - æ”¯æŒå¤šä¸ªåºåˆ—å¹¶è¡Œ"""
        batch_size = len(input_ids_list)

        # Prefill é˜¶æ®µ - å¹¶è¡Œå¤„ç†æ‰€æœ‰prompt
        prefill_results = []
        for i, input_ids in enumerate(input_ids_list):
            seq_len = len(input_ids)
            query, key, value = self.model.encode(input_ids)

            # è·å– KV Cache åˆ†é…
            kv_slot = self.kv_cache.allocate(i, seq_len + max_new_tokens)

            # Prefill è®¡ç®—
            prefill_out = self.attention.unified_inference(
                query, key, value, seq_len
            )
            prefill_results.append(prefill_out)

            # æ›´æ–° KV Cache
            self.kv_cache.update(i, key, value, 0, seq_len)

        # Decode é˜¶æ®µ - é€tokenç”Ÿæˆ
        generated_tokens = [[] for _ in range(batch_size)]

        for step in range(max_new_tokens):
            decode_queries = []
            seq_lengths = []

            for i in range(batch_size):
                # è·å–ä¸‹ä¸€ä¸ªtokençš„query
                next_query = self.model.get_next_query(i, step)
                decode_queries.append(next_query)
                seq_lengths.append(len(input_ids_list[i]) + step)

            # æ‰¹é‡decode
            batch_decode_out = batch_decode_step(
                decode_queries, seq_lengths, self.kv_cache
            )

            # è§£ç å¹¶æ›´æ–°
            for i, decode_out in enumerate(batch_decode_out):
                next_token = self.model.decode(decode_out)
                generated_tokens[i].append(next_token)

                # æ›´æ–° KV Cache
                next_kv = self.model.get_kv(i, step + 1)
                self.kv_cache.update(i, next_kv[0], next_kv[1],
                                   len(input_ids_list[i]) + step, 1)

        return generated_tokens

# æ€§èƒ½æ”¶ç›Š
# - ååé‡: ç›¸æ¯”CPUæå‡ 8-12x
# - å†…å­˜: KV Cacheå ç”¨é™ä½60%
# - å»¶è¿Ÿ: é¦–tokenå»¶è¿Ÿ-40%, åç»­tokenå»¶è¿Ÿ-70%
```

### 5.2 å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒ

#### åœºæ™¯ç‰¹ç‚¹
- è§†è§‰-è¯­è¨€è”åˆè®­ç»ƒ
- ä¸åŒæ¨¡æ€çš„æ³¨æ„åŠ›æ¨¡å¼å·®å¼‚
- å†…å­˜éœ€æ±‚å¤§

#### å®ç°æ–¹æ¡ˆ
```python
class MultimodalAttentionTrainer:
    """å¤šæ¨¡æ€æ³¨æ„åŠ›è®­ç»ƒå™¨"""

    def __init__(self):
        self.text_attention = NPUAttentionEngine(num_heads=12, head_dim=64)
        self.vision_attention = NPUAttentionEngine(num_heads=16, head_dim=64)

    def forward(self, text_input, vision_input):
        """å¤šæ¨¡æ€å‰å‘ä¼ æ’­"""
        # æ–‡æœ¬åˆ†æ”¯ - æ ‡å‡†å› æœæ³¨æ„åŠ›
        text_qkv = self.text_projection(text_input)
        text_output = torch_npu.npu_fusion_attention(
            *text_qkv,
            head_num=12,
            input_layout="BNSD",
            scale=1.0/math.sqrt(64),
            keep_prob=0.1,  # è®­ç»ƒdropout
            sparse_mode=3  # causal mask
        )[0]

        # è§†è§‰åˆ†æ”¯ - å±€éƒ¨æ³¨æ„åŠ›æ›´é€‚åˆå›¾åƒ
        vision_qkv = self.vision_projection(vision_input)
        vision_output = torch_npu.npu_fusion_attention(
            *vision_qkv,
            head_num=16,
            input_layout="BNSD",
            scale=1.0/math.sqrt(64),
            keep_prob=0.1,
            sparse_mode=4  # band attention for local features
        )[0]

        # å¤šæ¨¡æ€èåˆ
        fused_output = self.fusion_layer(text_output, vision_output)
        return fused_output

# è®­ç»ƒæ•ˆæœ
# - é€Ÿåº¦: ç›¸æ¯”æ ‡å‡†æ³¨æ„åŠ›æå‡ 3.5x
# - æ˜¾å­˜: å‡å°‘45%ï¼Œæ”¯æŒæ›´å¤§batch
# - æ”¶æ•›æ€§: æ•°å€¼ç²¾åº¦ä¸€è‡´
```

### 5.3 é•¿æ–‡æœ¬å¤„ç†

#### åœºæ™¯æŒ‘æˆ˜
- åºåˆ—é•¿åº¦ 8K-32K tokens
- å†…å­˜éœ€æ±‚ O(NÂ²) å¢é•¿
- éœ€è¦é«˜æ•ˆå‹ç¼©ç­–ç•¥

#### è§£å†³æ–¹æ¡ˆ
```python
class LongTextProcessor:
    """é•¿æ–‡æœ¬å¤„ç†å™¨"""

    def __init__(self, seq_len_threshold=4096):
        self.threshold = seq_len_threshold

    def adaptive_attention(self, query, key, value):
        """è‡ªé€‚åº”æ³¨æ„åŠ›ç­–ç•¥"""
        seq_len = query.shape[2] if query.dim() == 4 else query.shape[1]

        if seq_len <= self.threshold:
            # çŸ­åºåˆ—: æ ‡å‡†å› æœæ³¨æ„åŠ›
            return self.standard_attention(query, key, value)
        else:
            # é•¿åºåˆ—: åˆ†å±‚å¤„ç†
            return self.hierarchical_attention(query, key, value, seq_len)

    def hierarchical_attention(self, query, key, value, seq_len):
        """åˆ†å±‚æ³¨æ„åŠ› - å¤„ç†è¶…é•¿åºåˆ—"""
        chunk_size = self.threshold // 2
        num_chunks = (seq_len + chunk_size - 1) // chunk_size

        outputs = []

        for i in range(num_chunks):
            start = i * chunk_size
            end = min((i + 1) * chunk_size, seq_len)

            # å±€éƒ¨æ³¨æ„åŠ›
            local_q, local_k, local_v = self.extract_chunk(query, key, value, start, end)
            local_out = torch_npu.npu_fusion_attention(
                local_q, local_k, local_v,
                head_num=self.num_heads,
                input_layout="BNSD",
                scale=self.scale,
                sparse_mode=4  # band attention for local context
            )[0]

            outputs.append(local_out)

        # å…¨å±€æ‘˜è¦æ³¨æ„åŠ›
        if num_chunks > 1:
            global_out = self.global_summary_attention(outputs)
            return torch.cat([global_out] + outputs[1:], dim=2)

        return torch.cat(outputs, dim=2)

# æ€§èƒ½ä¼˜åŒ–
# - å†…å­˜: ä»O(NÂ²)é™è‡³O(N)
# - é€Ÿåº¦: é•¿åºåˆ—å¤„ç†æå‡5-8x
# - ç²¾åº¦: ä¿æŒä¸å®Œæ•´æ³¨æ„åŠ›ç›¸å½“
```

## 6. å¸¸è§é—®é¢˜ä¸æ•…éšœæ’é™¤

### 6.1 åŸºç¡€é—®é¢˜è¯Šæ–­

#### é—®é¢˜1: è¾“å…¥å½¢çŠ¶ä¸åŒ¹é…
```python
# é”™è¯¯ç¤ºä¾‹
query = torch.randn(2, 8, 512, 64)  # BNSD
out = torch_npu.npu_fusion_attention(
    query, key, value,
    input_layout="BSND"  # é”™è¯¯: ä¸å®é™…å¸ƒå±€ä¸åŒ¹é…
)

# æ­£ç¡®è§£å†³
out = torch_npu.npu_fusion_attention(
    query, key, value,
    input_layout="BNSD"  # åŒ¹é…å®é™…æ•°æ®å¸ƒå±€
)
```

#### é—®é¢˜2: æ•°æ®ç±»å‹ä¸ä¸€è‡´
```python
# é”™è¯¯ç¤ºä¾‹
query = torch.randn(..., dtype=torch.float16).npu()
key = torch.randn(..., dtype=torch.float32).npu()  # ç±»å‹ä¸åŒ¹é…

# æ­£ç¡®è§£å†³
key = key.to(torch.float16)  # ç»Ÿä¸€æ•°æ®ç±»å‹
value = value.to(torch.float16)
```

#### é—®é¢˜3: å†…å­˜æº¢å‡º(OOM)
```python
def handle_oom(query, key, value, **kwargs):
    """OOMå¤„ç†ç­–ç•¥"""
    try:
        return torch_npu.npu_fusion_attention(query, key, value, **kwargs)
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            # ç­–ç•¥1: å‡å°æ‰¹é‡å¤§å°
            if query.shape[0] > 1:
                smaller_batch = query.shape[0] // 2
                return process_in_chunks(
                    query[:smaller_batch], key[:smaller_batch],
                    value[:smaller_batch], **kwargs
                )

            # ç­–ç•¥2: æ¢¯åº¦æ£€æŸ¥ç‚¹
            return torch.utils.checkpoint.checkpoint(
                torch_npu.npu_fusion_attention,
                query, key, value,
                use_reentrant=False, **kwargs
            )
```

### 6.2 æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

```python
OPTIMIZATION_CHECKLIST = {
    "æ•°æ®å¸ƒå±€": "ä½¿ç”¨ BNSD æˆ– TND å¸ƒå±€ï¼Œé¿å… BSH",
    "ç»´åº¦å¯¹é½": "head_dim è®¾ä¸º16çš„å€æ•°(64/128/256)",
    "ç¨€ç–æ¨¡å¼": "æ˜ç¡®æŒ‡å®š sparse_modeï¼Œé¿å…bool mask",
    "é‡åŒ–æ¨ç†": "å†…å­˜å—é™æ—¶ä½¿ç”¨ INT8/FP8 é‡åŒ–",
    "PageAttention": "é«˜å¹¶å‘åœºæ™¯å¯ç”¨åˆ†å—ç®¡ç†",
    "APIé€‰æ‹©": "æ¨ç†åœºæ™¯ä¼˜å…ˆä½¿ç”¨ fused_infer_attention_score",
    "å¼‚æ­¥æ‰§è¡Œ": "è®¾ç½® sync=False æå‡ååé‡",
    "å†…å­˜å¤ç”¨": "KV Cache å¤ç”¨ï¼Œé¿å…é‡å¤è®¡ç®—"
}

def verify_optimization(query, key, value, config):
    """ä¼˜åŒ–éªŒè¯å‡½æ•°"""
    issues = []

    # æ£€æŸ¥å¸ƒå±€
    if config.get('input_layout') == 'BSH':
        issues.append("å»ºè®®ä½¿ç”¨ BNSD å¸ƒå±€è·å¾—æ›´å¥½æ€§èƒ½")

    # æ£€æŸ¥ç»´åº¦å¯¹é½
    head_dim = query.shape[-1]
    if head_dim % 16 != 0:
        issues.append(f"head_dim={head_dim}æœª16å¯¹é½ï¼Œå»ºè®®å¡«å……åˆ°{(head_dim//16+1)*16}")

    # æ£€æŸ¥ç¨€ç–æ¨¡å¼
    if config.get('sparse_mode') is None and config.get('atten_mask') is None:
        issues.append("æœªæŒ‡å®š sparse_modeï¼Œå¯èƒ½å½±å“æ€§èƒ½")

    return issues
```

### 6.3 ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜

#### APIå¯ç”¨æ€§æ£€æŸ¥
```python
def check_api_availability():
    """æ£€æŸ¥å½“å‰ç¯å¢ƒæ”¯æŒçš„API"""
    import torch_npu

    available_apis = []

    # æ£€æŸ¥åŸºç¡€API
    if hasattr(torch_npu, 'npu_fusion_attention'):
        available_apis.append('npu_fusion_attention')

    if hasattr(torch_npu, 'npu_incre_flash_attention'):
        available_apis.append('npu_incre_flash_attention')

    # æ£€æŸ¥æ–°API
    if hasattr(torch_npu, 'npu_fused_infer_attention_score'):
        available_apis.append('npu_fused_infer_attention_score')

    if hasattr(torch_npu, 'npu_prompt_flash_attention'):
        available_apis.append('npu_prompt_flash_attention')

    if hasattr(torch_npu, 'npu_advance_step_flashattn'):
        available_apis.append('npu_advance_step_flashattn')

    return available_apis

# ä½¿ç”¨ç¤ºä¾‹
available = check_api_availability()
print(f"æ”¯æŒçš„API: {available}")

if 'npu_fused_infer_attention_score' in available:
    print("æ¨èä½¿ç”¨ç»Ÿä¸€æ¨ç†æ¥å£")
elif 'npu_incre_flash_attention' in available:
    print("ä½¿ç”¨å¢é‡æ¨ç†æ¥å£")
else:
    print("ä»…æ”¯æŒåŸºç¡€è®­ç»ƒæ¥å£")
```

## 7. æœ€ä½³å®è·µä¸éƒ¨ç½²æŒ‡å—

### 7.1 APIé€‰æ‹©å†³ç­–æ ‘

```python
def choose_optimal_api(use_case, pytorch_version, environment="production"):
    """APIé€‰æ‹©å†³ç­–å™¨"""

    # è®­ç»ƒåœºæ™¯
    if use_case == "training":
        return "npu_fusion_attention"

    # æ¨ç†åœºæ™¯
    elif use_case == "inference":
        if pytorch_version >= "2.3":
            return "npu_fused_infer_attention_score"  # æœ€ä¼˜é€‰æ‹©
        else:
            return "npu_incre_flash_attention"  # å…¼å®¹é€‰æ‹©

    # vLLMé›†æˆ
    elif use_case == "vllm":
        if pytorch_version >= "2.5":
            return "npu_advance_step_flashattn"
        else:
            raise ValueError("vLLMéœ€è¦PyTorch 2.5+æ”¯æŒ")

    # ç ”ç©¶å¼€å‘
    elif use_case == "research":
        if pytorch_version >= "2.3":
            return "npu_fused_infer_attention_score"  # åŠŸèƒ½æœ€å…¨
        else:
            return "npu_incre_flash_attention"  # åŸºç¡€ç¨³å®š

    else:
        raise ValueError(f"æœªçŸ¥ä½¿ç”¨åœºæ™¯: {use_case}")

# é…ç½®ç”Ÿæˆå™¨
def get_optimal_config(api_name, use_case):
    """è·å–æœ€ä¼˜é…ç½®å‚æ•°"""
    configs = {
        "npu_fusion_attention": {
            "training": {"keep_prob": 0.9, "sparse_mode": 3, "inner_precise": 1},
            "inference": {"keep_prob": 1.0, "sparse_mode": 3, "inner_precise": 0}
        },
        "npu_fused_infer_attention_score": {
            "inference": {"sparse_mode": 3, "softmax_lse_flag": True}
        },
        "npu_incre_flash_attention": {
            "inference": {"sync": False, "inner_precise": 0}
        }
    }

    return configs.get(api_name, {}).get(use_case, {})
```

### 7.2 ç”Ÿäº§ç¯å¢ƒç›‘æ§

```python
class ProductionMonitor:
    """ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ç›‘æ§"""

    def __init__(self):
        self.metrics = {
            "total_calls": 0,
            "total_time": 0.0,
            "memory_peak": 0.0,
            "error_count": 0,
            "oom_count": 0
        }

    def monitored_attention(self, query, key, value, api_func, **kwargs):
        """å¸¦ç›‘æ§çš„æ³¨æ„åŠ›è®¡ç®—"""
        import time
        start_time = time.perf_counter()
        start_memory = torch.npu.max_memory_allocated()

        try:
            result = api_func(query, key, value, **kwargs)

            # æ›´æ–°æˆåŠŸæŒ‡æ ‡
            self.metrics["total_calls"] += 1
            self.metrics["total_time"] += time.perf_counter() - start_time
            current_memory = torch.npu.max_memory_allocated() - start_memory
            self.metrics["memory_peak"] = max(self.metrics["memory_peak"], current_memory)

            return result

        except RuntimeError as e:
            self.metrics["error_count"] += 1
            if "out of memory" in str(e).lower():
                self.metrics["oom_count"] += 1
            raise e

    def get_health_report(self):
        """ç”Ÿæˆå¥åº·æŠ¥å‘Š"""
        if self.metrics["total_calls"] == 0:
            return {"status": "no_data", "message": "æš‚æ— è°ƒç”¨è®°å½•"}

        avg_time = self.metrics["total_time"] / self.metrics["total_calls"]
        error_rate = self.metrics["error_count"] / self.metrics["total_calls"]
        oom_rate = self.metrics["oom_count"] / self.metrics["total_calls"]

        # å¥åº·çŠ¶æ€åˆ¤æ–­
        if error_rate > 0.05:  # é”™è¯¯ç‡>5%
            status = "unhealthy"
        elif oom_rate > 0.01:  # OOMç‡>1%
            status = "warning"
        elif avg_time > 10.0:  # å¹³å‡è€—æ—¶>10ms
            status = "warning"
        else:
            status = "healthy"

        return {
            "status": status,
            "metrics": {
                "total_calls": self.metrics["total_calls"],
                "avg_time_ms": round(avg_time * 1000, 2),
                "memory_peak_mb": round(self.metrics["memory_peak"] / 1024**2, 1),
                "error_rate": round(error_rate * 100, 2),
                "oom_rate": round(oom_rate * 100, 2)
            }
        }

# ç›‘æ§ä½¿ç”¨ç¤ºä¾‹
monitor = ProductionMonitor()

def safe_production_attention(query, key, value, **kwargs):
    """ç”Ÿäº§ç¯å¢ƒå®‰å…¨è°ƒç”¨"""
    return monitor.monitored_attention(
        query, key, value,
        torch_npu.npu_fused_infer_attention_score,
        **kwargs
    )
```

### 7.3 å®¹é”™ä¸é™çº§ç­–ç•¥

```python
class RobustAttentionEngine:
    """å¥å£®çš„æ³¨æ„åŠ›å¼•æ“"""

    def __init__(self, fallback_to_cpu=True, fallback_to_torch=True):
        self.fallback_to_cpu = fallback_to_cpu
        self.fallback_to_torch = fallback_to_torch

    def safe_attention(self, query, key, value, api_func, **kwargs):
        """å®‰å…¨çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œæ”¯æŒå¤šçº§é™çº§"""

        # ç¬¬ä¸€çº§: NPUåŸç”Ÿå®ç°
        try:
            return api_func(query, key, value, **kwargs)

        except RuntimeError as npu_error:
            print(f"NPUè®¡ç®—å¤±è´¥: {npu_error}")

            # ç¬¬äºŒçº§: é™çº§åˆ°CPU NPUå®ç°
            if self.fallback_to_cpu and "npu" in str(npu_error).lower():
                try:
                    query_cpu, key_cpu, value_cpu = query.cpu(), key.cpu(), value.cpu()
                    return api_func(query_cpu, key_cpu, value_cpu, **kwargs).to(query.device)
                except Exception as cpu_error:
                    print(f"é™çº§åˆ°CPUå¤±è´¥: {cpu_error}")

            # ç¬¬ä¸‰çº§: PyTorchæ ‡å‡†å®ç°
            if self.fallback_to_torch:
                try:
                    print("é™çº§åˆ°PyTorchæ ‡å‡†å®ç°")
                    return torch.nn.functional.scaled_dot_product_attention(
                        query, key, value, **kwargs
                    )
                except Exception as torch_error:
                    print(f"PyTorchå®ç°å¤±è´¥: {torch_error}")

            # æ— æ³•é™çº§ï¼Œé‡æ–°æŠ›å‡ºåŸå§‹å¼‚å¸¸
            raise npu_error

    def adaptive_batch_size(self, query, key, value, max_batch_size=32):
        """è‡ªé€‚åº”æ‰¹é‡å¤§å°ï¼Œé¿å…OOM"""
        original_batch = query.shape[0]

        for batch_size in range(min(max_batch_size, original_batch), 0, -1):
            try:
                if batch_size == original_batch:
                    return self.safe_attention(query, key, value, torch_npu.npu_fusion_attention)
                else:
                    # åˆ†æ‰¹å¤„ç†
                    outputs = []
                    for i in range(0, original_batch, batch_size):
                        end = min(i + batch_size, original_batch)
                        batch_output = self.safe_attention(
                            query[i:end], key[i:end], value[i:end],
                            torch_npu.npu_fusion_attention
                        )
                        outputs.append(batch_output)
                    return torch.cat(outputs, dim=0)

            except RuntimeError as e:
                if "out of memory" in str(e).lower() and batch_size > 1:
                    continue  # å°è¯•æ›´å°çš„æ‰¹é‡
                else:
                    raise e

        raise RuntimeError("æ— æ³•æ‰¾åˆ°åˆé€‚çš„æ‰¹é‡å¤§å°")
```

## 8. æœªæ¥å‘å±•ä¸è¶‹åŠ¿

### 8.1 æŠ€æœ¯æ¼”è¿›æ–¹å‘

| æ–¹å‘ | å½“å‰çŠ¶æ€ | å‘å±•ç›®æ ‡ | å½±å“åŠ› |
|------|----------|----------|--------|
| **ç²¾åº¦æ”¯æŒ** | FP16/BF16 | FP8/INT4é‡åŒ– | å†…å­˜è¿›ä¸€æ­¥ä¼˜åŒ– |
| **åºåˆ—é•¿åº¦** | 32K tokens | 100K+ tokens | æ”¯æŒé•¿æ–‡æ¡£å¤„ç† |
| **å»¶è¿Ÿä¼˜åŒ–** | 2-5ms/token | <1ms/token | å®æ—¶åº”ç”¨æ”¯æŒ |
| **æ¡†æ¶é›†æˆ** | PyTorch | å¤šæ¡†æ¶ç»Ÿä¸€ | é™ä½ä½¿ç”¨é—¨æ§› |

### 8.2 åº”ç”¨åœºæ™¯æ‰©å±•

#### æ–°å…´åº”ç”¨é¢†åŸŸ
- **å¤šæ¨¡æ€èåˆ**: è§†è§‰-è¯­è¨€-éŸ³é¢‘ç»Ÿä¸€æ³¨æ„åŠ›
- **ç§‘å­¦è®¡ç®—**: éåºåˆ—æ•°æ®çš„æ³¨æ„åŠ›å»ºæ¨¡
- **è¾¹ç¼˜è®¡ç®—**: è½»é‡åŒ–æ¨¡å‹é«˜æ•ˆæ¨ç†
- **è”é‚¦å­¦ä¹ **: åˆ†å¸ƒå¼æ³¨æ„åŠ›éšç§ä¿æŠ¤

#### ç”Ÿæ€ç³»ç»Ÿå»ºè®¾
- **è‡ªåŠ¨åŒ–å·¥å…·**: æ€§èƒ½è°ƒä¼˜å’Œè¯Šæ–­å·¥å…·
- **ç¤¾åŒºæ”¯æŒ**: å¼€æºæ¨¡å‹é€‚é…å’Œç¤ºä¾‹
- **æ ‡å‡†åŒ–**: è·¨å¹³å°APIç»Ÿä¸€æ ‡å‡†

---

*æ–‡æ¡£æ¥æºå‚è€ƒ: [æ˜‡è…¾ç¤¾åŒºå®˜æ–¹æ–‡æ¡£](https://www.hiascend.com/document/detail/zh/Pytorch/)*
- torch_npu.npu_fusion_attention (60RC1)
- torch_npu.npu_prompt_flash_attention (700)
- torch_npu.npu_incre_flash_attention (60RC3)
- torch_npu.npu_fused_infer_attention_score (600)
- torch_npu.npu_advance_step_flashattn (700)
